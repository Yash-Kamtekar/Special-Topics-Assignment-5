{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-Kamtekar/Special-Topics-Assignment-5/blob/main/active_learning_with_label_studio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "irHleUeIG7EY",
        "outputId": "9b219b19-f1e0-4d45-a3d2-2ee58e98117a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lightly\n",
            "  Downloading lightly-1.2.35-py3-none-any.whl (507 kB)\n",
            "\u001b[K     |████████████████████████████████| 507 kB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.8.2)\n",
            "Collecting lightly-utils~=0.0.0\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.21.6)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from lightly) (2022.9.24)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.24.3)\n",
            "Collecting pytorch-lightning>=1.0.4\n",
            "  Downloading pytorch_lightning-1.8.1-py3-none-any.whl (798 kB)\n",
            "\u001b[K     |████████████████████████████████| 798 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.0.0\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from lightly) (0.13.1+cu113)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.7/dist-packages (from lightly) (4.64.1)\n",
            "Collecting omegaconf~=2.2\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.0.0->lightly) (21.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.0.0->lightly) (5.10.0)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from lightly-utils~=0.0.0->lightly) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf~=2.2->hydra-core>=1.0.0->lightly) (6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (2.9.1)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (1.12.1+cu113)\n",
            "Collecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (2022.10.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (4.1.1)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (3.8.3)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (4.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->hydra-core>=1.0.0->lightly) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (3.0.4)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.50.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.38.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning>=1.0.4->lightly) (2.1.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, fire\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144576 sha256=447978cb6bc2f4d08a42b1bb928ca6accb0afad3bb3e4163a1b7c9f9ec8b3deb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115940 sha256=6fb2e0fcef96806585b9d0e171a02b3321483c664f9915ff8f84eec087a5d71d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built antlr4-python3-runtime fire\n",
            "Installing collected packages: fire, antlr4-python3-runtime, torchmetrics, omegaconf, lightning-utilities, pytorch-lightning, lightly-utils, hydra-core, lightly\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 fire-0.4.0 hydra-core-1.2.0 lightly-1.2.35 lightly-utils-0.0.2 lightning-utilities-0.3.0 omegaconf-2.2.3 pytorch-lightning-1.8.1 torchmetrics-0.10.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install lightly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPyEoWPCHCKC",
        "outputId": "9bfd5488-c8e1-4711-8752-f00415c6f814"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, io, models, ops, transforms, utils\n",
        "\n",
        "\n",
        "import json\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "\n",
        "def read_label_element(label_element: Dict) -> Tuple[str, str]:\n",
        "    filepath = \"/\" + label_element[\"data\"][\"image\"].split(\"?d=\")[-1]\n",
        "    label = label_element[\"annotations\"][0][\"result\"][0][\"value\"][\"rectanglelabels\"][0]\n",
        "    return filepath, label\n",
        "\n",
        "def read_LabelStudio_label_file(filepath: str) -> Tuple[List[str],List[str]]:\n",
        "    # read the label file\n",
        "    with open(filepath, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    filepaths, labels = zip(* [read_label_element(label_element) for label_element in data])\n",
        "    return filepaths, labels"
      ],
      "metadata": {
        "id": "TC8SPgMJHEZV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from lightly.active_learning.scorers import ScorerClassification\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms as T\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PQt3dSt3HJfp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lightly-magic token='7d9e3e942dbb8949325f955a838e79d822c3405cff6e0444' dataset_id='637491f296ba509f0cd0aa90' input_dir='/content/train' trainer.max_epochs=20"
      ],
      "metadata": {
        "id": "8lAe-TXHHMkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe23c11-b163-4351-bc7b-32f812cdd01a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py:127: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  configure_logging=with_log_configuration,\n",
            "########## Starting to train an embedding model.\n",
            "/usr/local/lib/python3.7/dist-packages/lightly/cli/train_cli.py:69: UserWarning: Training a self-supervised model with a small batch size: 16! Small batch size may harm embedding quality. You can specify the batch size via the loader key-word: loader.batch_size=BSZ\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://storage.googleapis.com/models_boris/whattolabel-resnet18-simclr-d32-w1.0-i-085d0693.pth\" to /root/.cache/torch/hub/checkpoints/whattolabel-resnet18-simclr-d32-w1.0-i-085d0693.pth\n",
            "100% 42.8M/42.8M [00:06<00:00, 6.90MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: /content/lightly_outputs/2022-11-16/07-33-41/lightning_logs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:601: UserWarning: Checkpoint directory /content/lightly_outputs/2022-11-16/07-33-41 exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type       | Params\n",
            "-----------------------------------------\n",
            "0 | model     | _SimCLR    | 11.2 M\n",
            "1 | criterion | NTXentLoss | 0     \n",
            "-----------------------------------------\n",
            "11.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "11.2 M    Total params\n",
            "44.762    Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1562: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  category=PossibleUserWarning,\n",
            "Epoch 2:  78% 14/18 [00:02<00:00,  6.10it/s, loss=3.37, v_num=0][2022-11-16 07:34:12,038][numexpr.utils][INFO] - NumExpr defaulting to 2 threads.\n",
            "Epoch 19: 100% 18/18 [00:03<00:00,  4.87it/s, loss=3.3, v_num=0]`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "Epoch 19: 100% 18/18 [00:03<00:00,  4.54it/s, loss=3.3, v_num=0]\n",
            "Best model is stored at: \u001b[94m/content/lightly_outputs/2022-11-16/07-33-41/lightly_epoch_17.ckpt\u001b[0m\n",
            "########## Starting to embed your dataset.\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Compute efficiency: 0.70: 100% 299/299 [00:00<00:00, 316.85imgs/s]\n",
            "Embeddings are stored at \u001b[94m/content/lightly_outputs/2022-11-16/07-33-41/embeddings.csv\u001b[0m\n",
            "########## Starting to upload your dataset to the Lightly platform.\n",
            "Uploading \u001b[92m299\u001b[0m images (with \u001b[92m2\u001b[0m workers).\n",
            "100% 299/299 [08:02<00:00,  1.61s/imgs]\n",
            "Finished the upload of the dataset.\n",
            "Finished upload of embeddings.\n",
            "########## Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchImageDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_paths: List[str],\n",
        "        label_names: List[str],\n",
        "        transform: object = None,\n",
        "        labels: List[str] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param image_paths:\n",
        "            one path for each image\n",
        "        :param label_names:\n",
        "            an ordered list of possible labels\n",
        "        :param transform:\n",
        "        :param labels:\n",
        "            one label for each image. Each label must be in the label_names.\n",
        "        \"\"\"\n",
        "        if transform is None:\n",
        "            transform = T.Compose(\n",
        "                [\n",
        "                    T.Resize((360, 117)),\n",
        "                    T.ToTensor(),\n",
        "                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ]\n",
        "            )\n",
        "        self.transform = transform\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "\n",
        "        self.label_names = label_names\n",
        "        self.label_name_to_index = {name: i for i, name in enumerate(label_names)}\n",
        "\n",
        "    def __getitem__(self, index: int) -> Union[Tuple[torch.Tensor, int], torch.Tensor]:\n",
        "        image_path = self.image_paths[index]\n",
        "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "        image_torch = self.transform(image_pil)\n",
        "        if self.labels:\n",
        "            return image_torch, self.label_name_to_index[self.labels[index]]\n",
        "        else:\n",
        "            return image_torch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "class ClassificationModel:\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_classes: int = 8, \n",
        "        no_epochs: int = 5, \n",
        "        num_workers: int = None,\n",
        "        batch_size: int = 1,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # don't forget to initialize base class...\n",
        "        self.model = torchvision.models.resnet18(\n",
        "            pretrained=False, progress=True, num_classes=num_classes\n",
        "        )\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(self.device)\n",
        "        self.no_epochs = no_epochs\n",
        "        self.model_is_trained = False\n",
        "        self.num_workers = num_workers or min(8, os.cpu_count())\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def save_on_disk(self, model_path: str = \"./classifier.pth\"):\n",
        "        to_save = {\"model\": self.model.to(\"cpu\"), \"label_names\": self.label_names}\n",
        "        torch.save(to_save, model_path)\n",
        "\n",
        "    def load_from_disk(self, model_path: str = \"./classifier.pth\"):\n",
        "        saved_data = torch.load(model_path)\n",
        "        self.label_names = saved_data[\"label_names\"]\n",
        "        self.model = saved_data[\"model\"].to(self.device)\n",
        "        self.model_is_trained = True\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        image_paths: List[str],\n",
        "        image_labels: List[str],\n",
        "        label_names: List[str] = None,\n",
        "    ):\n",
        "        print(\"STARTING FITTING\")\n",
        "        if label_names is None:\n",
        "            self.label_names = sorted(list(set(image_labels)))\n",
        "\n",
        "        dataset = TorchImageDataset(\n",
        "            image_paths=image_paths, label_names=self.label_names, labels=image_labels\n",
        "        )\n",
        "        \n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, \n",
        "            batch_size=self.batch_size, \n",
        "            shuffle=True, \n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
        "        self.model.train()\n",
        "        pbar = tqdm(range(self.no_epochs), file=sys.stdout)\n",
        "        for epoch in pbar:  # loop over the dataset multiple times\n",
        "            running_loss = 0.0\n",
        "            total_samples = 0\n",
        "            correct = 0\n",
        "            for data in dataloader:\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item() * labels.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_samples += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            text = f\"epoch: {epoch} loss: {running_loss / total_samples:.6f} accuracy: {correct/total_samples:.3f}\"\n",
        "            tqdm.write(text)\n",
        "\n",
        "        self.model_is_trained = True\n",
        "        print(\"FINISHED FITTING\")\n",
        "\n",
        "    def predict(self, image_paths: List[str]) -> Tuple[List[str], ScorerClassification]:\n",
        "        print(\"STARTING PREDICTION\")\n",
        "\n",
        "        if not self.model_is_trained:\n",
        "            raise ValueError\n",
        "\n",
        "        dataset = TorchImageDataset(image_paths, self.label_names)\n",
        "\n",
        "        dataloader = DataLoader(\n",
        "            dataset, \n",
        "            batch_size=self.batch_size, \n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        predictions = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x in tqdm(dataloader):\n",
        "                pred = self.model(x.to(self.device)).cpu()\n",
        "                predictions.append(pred)\n",
        "\n",
        "        print(\"PUTTING TOGETHER RETURN VALUES\")\n",
        "\n",
        "        # flatten over batches\n",
        "        predictions = [i for sublist in predictions for i in sublist]\n",
        "        predictions = torch.stack(predictions, dim=0)\n",
        "        predictions = torch.nn.functional.softmax(predictions, dim=1)\n",
        "\n",
        "        predicted_classes_int = torch.argmax(predictions, dim=1)\n",
        "        predicted_classes_str = [self.label_names[i] for i in predicted_classes_int]\n",
        "        scorer = ScorerClassification(predictions)\n",
        "\n",
        "        print(\"FINISHED PREDICTION\")\n",
        "\n",
        "        return predicted_classes_str, scorer"
      ],
      "metadata": {
        "id": "gnChIr-THNIO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from lightly.active_learning.agents import ActiveLearningAgent\n",
        "from lightly.active_learning.config import SelectionConfig\n",
        "from lightly.api import ApiWorkflowClient\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.openapi_generated.swagger_client import SamplingMethod\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    path_full_dataset = \"/content/train\" ## UPDATE THIS PATH\n",
        "\n",
        "    # 1. Prepare the active learning agent to make sure that your credentials are correct\n",
        "    lighty_webapp_token = \"7d9e3e942dbb8949325f955a838e79d822c3405cff6e0444\"\n",
        "    lighty_webapp_dataset_id = \"637491f296ba509f0cd0aa90\"\n",
        "    api_workflow_client = ApiWorkflowClient(token=lighty_webapp_token, dataset_id=lighty_webapp_dataset_id)\n",
        "    al_agent = ActiveLearningAgent(api_workflow_client=api_workflow_client)\n",
        "\n",
        "    # 2. read the label file\n",
        "    filepaths, labels = read_LabelStudio_label_file(\"/content/jsonformat.json\") ##UPDATE THIS PATH\n",
        "\n",
        "    # 3. Define the image classification model\n",
        "    classifier = ClassificationModel(no_epochs=20)\n",
        "\n",
        "    # 4. Fit the classifier on the labeled images\n",
        "    classifier.fit(image_paths=filepaths, image_labels=labels)\n",
        "\n",
        "    # 5. Predict with the classifier on the complete dataset\n",
        "    image_filenames_full_dataset = LightlyDataset(path_full_dataset).get_filenames()\n",
        "    image_paths_full_dataset = [os.path.join(path_full_dataset, filename) for filename in image_filenames_full_dataset]\n",
        "    predicted_classes_str, scorer = classifier.predict(image_paths=image_paths_full_dataset)\n",
        "\n",
        "    # 6. Use the active learning sampler \"CORAL\" via the Lightly API to sample until we have 45 samples\n",
        "    sampler_config = SelectionConfig(method=SamplingMethod.CORAL, n_samples=45, name=\"datasets\")\n",
        "    al_agent.query(sampler_config, al_scorer=scorer)\n"
      ],
      "metadata": {
        "id": "iC2pgGfqHO5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cdb621-8df4-49dd-9612-5e93b2e6c45d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING FITTING\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4d6d3c93b0>Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f4d6d3c93b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    self._shutdown_workers()    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
            "    if w.is_alive():AssertionError\n",
            ":   File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "can only test a child process\n",
            "    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7f4d6d3c93b0>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "AssertionError  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
            ":     can only test a child processself._shutdown_workers()\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
            "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4d6d3c93b0>if w.is_alive():\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionErrorself._shutdown_workers()\n",
            ":   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
            "can only test a child process    \n",
            "if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 loss: 1.967100 accuracy: 0.333\n",
            "epoch: 1 loss: 2.685572 accuracy: 0.056\n",
            "epoch: 2 loss: 2.571573 accuracy: 0.167\n",
            "epoch: 3 loss: 2.138093 accuracy: 0.111\n",
            "epoch: 4 loss: 2.041456 accuracy: 0.222\n",
            "epoch: 5 loss: 2.309763 accuracy: 0.278\n",
            "epoch: 6 loss: 1.261855 accuracy: 0.500\n",
            "epoch: 7 loss: 1.871713 accuracy: 0.167\n",
            "epoch: 8 loss: 1.125784 accuracy: 0.444\n",
            "epoch: 9 loss: 0.822802 accuracy: 0.778\n",
            "epoch: 10 loss: 0.613614 accuracy: 0.944\n",
            "epoch: 11 loss: 0.459094 accuracy: 1.000\n",
            "epoch: 12 loss: 0.309658 accuracy: 1.000\n",
            "epoch: 13 loss: 0.272038 accuracy: 1.000\n",
            "epoch: 14 loss: 0.153584 accuracy: 1.000\n",
            "epoch: 15 loss: 0.102112 accuracy: 1.000\n",
            "epoch: 16 loss: 0.084394 accuracy: 1.000\n",
            "epoch: 17 loss: 0.067493 accuracy: 1.000\n",
            "epoch: 18 loss: 0.057572 accuracy: 1.000\n",
            "epoch: 19 loss: 0.050049 accuracy: 1.000\n",
            "100%|██████████| 20/20 [00:09<00:00,  2.09it/s]\n",
            "FINISHED FITTING\n",
            "STARTING PREDICTION\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 299/299 [00:02<00:00, 135.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PUTTING TOGETHER RETURN VALUES\n",
            "FINISHED PREDICTION\n"
          ]
        }
      ]
    }
  ]
}